{
  "$schema": "../../schema/locale.schema.json",
  "locale": "pl",
  "category": "ai",
  "version": "1.0.0",
  "translations": {
    "modules.ai.embed.description": "Generowanie wektorów osadzania z tekstu za pomocą modeli AI",
    "modules.ai.embed.label": "Osadzanie Tekstu",
    "modules.ai.embed.output.dimensions.description": "Liczba wymiarów wektora osadzania",
    "modules.ai.embed.output.embeddings.description": "Tablica wektorów osadzania",
    "modules.ai.embed.output.model.description": "Model używany do osadzania",
    "modules.ai.embed.output.token_count.description": "Liczba przetworzonych tokenów",
    "modules.ai.embed.params.api_key.description": "Klucz API (domyślnie zmienna środowiskowa)",
    "modules.ai.embed.params.api_key.label": "Klucz API",
    "modules.ai.embed.params.dimensions.description": "Wymiary osadzania (dla modeli, które to obsługują)",
    "modules.ai.embed.params.dimensions.label": "Wymiary",
    "modules.ai.embed.params.model.description": "Model osadzania do użycia",
    "modules.ai.embed.params.model.label": "Model",
    "modules.ai.embed.params.provider.description": "Dostawca AI dla osadzania",
    "modules.ai.embed.params.provider.label": "Dostawca",
    "modules.ai.embed.params.text.description": "Tekst do osadzenia",
    "modules.ai.embed.params.text.label": "Tekst",
    "modules.ai.extract.description": "Wyodrębnianie danych strukturalnych z tekstu za pomocą AI",
    "modules.ai.extract.label": "Wyodrębnianie AI",
    "modules.ai.extract.output.extracted.description": "Wyodrębnione dane strukturalne",
    "modules.ai.extract.output.model.description": "Model używany do wyodrębniania",
    "modules.ai.extract.output.raw_response.description": "Surowa odpowiedź modelu",
    "modules.ai.extract.params.api_key.description": "Klucz API (domyślnie zmienna środowiskowa)",
    "modules.ai.extract.params.api_key.label": "Klucz API",
    "modules.ai.extract.params.instructions.description": "Dodatkowe instrukcje wyodrębniania",
    "modules.ai.extract.params.instructions.label": "Instrukcje",
    "modules.ai.extract.params.model.description": "Model do użycia do wyodrębniania",
    "modules.ai.extract.params.model.label": "Model",
    "modules.ai.extract.params.provider.description": "Dostawca AI do użycia",
    "modules.ai.extract.params.provider.label": "Dostawca",
    "modules.ai.extract.params.schema.description": "Schemat JSON definiujący pola do wyodrębnienia",
    "modules.ai.extract.params.schema.label": "Schemat",
    "modules.ai.extract.params.temperature.description": "Temperatura próbkowania (0-2)",
    "modules.ai.extract.params.temperature.label": "Temperatura",
    "modules.ai.extract.params.text.description": "Tekst, z którego wyodrębnia się dane",
    "modules.ai.extract.params.text.label": "Tekst",
    "modules.ai.local_ollama.chat.description": "Czat z lokalnym LLM przez Ollama (calkowicie offline)",
    "modules.ai.local_ollama.chat.label": "Lokalny czat Ollama",
    "modules.ai.local_ollama.chat.output.context.description": "Odpowiedz z operacji",
    "modules.ai.local_ollama.chat.output.eval_count.description": "Czas ladowania modelu",
    "modules.ai.local_ollama.chat.output.load_duration.description": "Kontekst rozmowy dla kolejnych zapytan",
    "modules.ai.local_ollama.chat.output.model.description": "Odpowiedz z operacji",
    "modules.ai.local_ollama.chat.output.prompt_eval_count.description": "Calkowity czas przetwarzania",
    "modules.ai.local_ollama.chat.output.response.description": "Maksymalna liczba tokenow w odpowiedzi (opcjonalne, zalezy od modelu)",
    "modules.ai.local_ollama.chat.output.total_duration.description": "Nazwa lub identyfikator modelu",
    "modules.ai.local_ollama.chat.params.max_tokens.description": "URL serwera Ollama",
    "modules.ai.local_ollama.chat.params.max_tokens.label": "URL Ollama",
    "modules.ai.local_ollama.chat.params.model.description": "Wiadomosc do wyslania do lokalnego LLM",
    "modules.ai.local_ollama.chat.params.model.label": "Prompt",
    "modules.ai.local_ollama.chat.params.ollama_url.description": "Wiadomosc roli systemowej (opcjonalne)",
    "modules.ai.local_ollama.chat.params.ollama_url.label": "Wiadomosc systemowa",
    "modules.ai.local_ollama.chat.params.prompt.description": "Wiadomosc do wyslania do lokalnego LLM",
    "modules.ai.local_ollama.chat.params.prompt.label": "Prompt",
    "modules.ai.local_ollama.chat.params.system_message.description": "Wiadomosc roli systemowej (opcjonalne)",
    "modules.ai.local_ollama.chat.params.system_message.label": "Temperatura",
    "modules.ai.local_ollama.chat.params.temperature.description": "Temperatura probkowania (0-2)",
    "modules.ai.local_ollama.chat.params.temperature.label": "CodeLlama 7B",
    "modules.ai.memory.description": "Pamiec rozmowy dla agenta AI",
    "modules.ai.memory.entity.description": "Wyodrebniaj i sledz encje (osoby, miejsca, pojecia) z rozmow",
    "modules.ai.memory.entity.label": "Pamiec encji",
    "modules.ai.memory.entity.output.config.description": "Sledzone encje wedlug typu",
    "modules.ai.memory.entity.output.entities.description": "Typ pamieci (encja)",
    "modules.ai.memory.entity.output.memory_type.description": "Maksymalna liczba encji do zapamietania",
    "modules.ai.memory.entity.output.relationships.description": "Identyfikator sesji",
    "modules.ai.memory.entity.output.session_id.description": "Maksymalna liczba encji do zapamietania",
    "modules.ai.memory.entity.params.entity_types": "Typy encji",
    "modules.ai.memory.entity.params.entity_types.options.concept": "Concept",
    "modules.ai.memory.entity.params.entity_types.options.location": "Location",
    "modules.ai.memory.entity.params.entity_types.options.organization": "Organization",
    "modules.ai.memory.entity.params.entity_types.options.person": "Person",
    "modules.ai.memory.entity.params.extraction_model": "Model ekstrakcji",
    "modules.ai.memory.entity.params.max_entities": "Sledz relacje",
    "modules.ai.memory.entity.params.session_id": "ID sesji",
    "modules.ai.memory.entity.params.track_relationships": "ID sesji",
    "modules.ai.memory.entity.ports.memory": "Typy encji",
    "modules.ai.memory.label": "Pamiec AI",
    "modules.ai.memory.options.buffer": "Typ pamieci",
    "modules.ai.memory.options.summary": "Rozmiar okna",
    "modules.ai.memory.options.window": "Typ pamieci",
    "modules.ai.memory.output.config.description": "Identyfikator sesji",
    "modules.ai.memory.output.memory_type.description": "Wstepnie zaladowana historia rozmowy",
    "modules.ai.memory.output.messages.description": "Typ pamieci",
    "modules.ai.memory.output.session_id.description": "Wstepnie zaladowana historia rozmowy",
    "modules.ai.memory.params.initial_messages": "ID sesji",
    "modules.ai.memory.params.initial_messages.description": "Wstepnie zaladowana historia rozmowy",
    "modules.ai.memory.params.memory_type": "Typ pamieci",
    "modules.ai.memory.params.memory_type.description": "Typ przechowywania pamieci",
    "modules.ai.memory.params.session_id": "ID sesji",
    "modules.ai.memory.params.session_id.description": "Unikalny identyfikator dla tej sesji rozmowy",
    "modules.ai.memory.params.window_size": "Rozmiar okna",
    "modules.ai.memory.params.window_size.description": "Liczba ostatnich wiadomosci do zachowania (dla pamieci okna)",
    "modules.ai.memory.ports.memory": "Typ pamieci",
    "modules.ai.memory.redis.description": "Trwala pamiec rozmowy z uzyciem magazynu Redis",
    "modules.ai.memory.redis.label": "Pamiec Redis",
    "modules.ai.memory.redis.output.config.description": "Zaladowana historia wiadomosci",
    "modules.ai.memory.redis.output.connected.description": "Identyfikator sesji",
    "modules.ai.memory.redis.output.memory_type.description": "Laduj istniejace wiadomosci z Redis przy inicjalizacji",
    "modules.ai.memory.redis.output.messages.description": "Typ pamieci (redis)",
    "modules.ai.memory.redis.output.session_id.description": "Laduj istniejace wiadomosci z Redis przy inicjalizacji",
    "modules.ai.memory.redis.params.key_prefix": "URL Redis",
    "modules.ai.memory.redis.params.load_on_start": "Maks. wiadomosci",
    "modules.ai.memory.redis.params.max_messages": "TTL (sekundy)",
    "modules.ai.memory.redis.params.redis_url": "URL Redis",
    "modules.ai.memory.redis.params.session_id": "Prefiks klucza",
    "modules.ai.memory.redis.params.ttl_seconds": "ID sesji",
    "modules.ai.memory.redis.ports.memory": "URL Redis",
    "modules.ai.memory.vector.description": "Pamiec semantyczna z uzyciem embeddingow wektorowych do pobierania odpowiedniego kontekstu",
    "modules.ai.memory.vector.label": "Pamiec wektorowa",
    "modules.ai.memory.vector.output.config.description": "Identyfikator sesji",
    "modules.ai.memory.vector.output.embedding_model.description": "Typ pamieci (wektor)",
    "modules.ai.memory.vector.output.memory_type.description": "Dolacz znacznik czasu i inne metadane do wspomnien",
    "modules.ai.memory.vector.output.session_id.description": "Dolacz znacznik czasu i inne metadane do wspomnien",
    "modules.ai.memory.vector.params.embedding_model": "Model embeddingu",
    "modules.ai.memory.vector.params.include_metadata": "ID sesji",
    "modules.ai.memory.vector.params.session_id": "Prog podobienstwa",
    "modules.ai.memory.vector.params.similarity_threshold": "Top K wynikow",
    "modules.ai.memory.vector.params.top_k": "Top K wynikow",
    "modules.ai.memory.vector.ports.memory": "Model embeddingu",
    "modules.ai.model.description": "Konfiguracja modelu LLM dla agenta AI",
    "modules.ai.model.label": "Model AI",
    "modules.ai.model.output.config.description": "Nazwa dostawcy LLM",
    "modules.ai.model.output.model.description": "Nazwa dostawcy LLM",
    "modules.ai.model.output.provider.description": "Maksymalna liczba tokenow w odpowiedzi",
    "modules.ai.model.params.max_tokens": "Maks. tokenow",
    "modules.ai.model.params.max_tokens.description": "Maksymalna liczba tokenow w odpowiedzi",
    "modules.ai.model.ports.model": "Model",
    "modules.ai.vision.analyze.description": "Analiza obrazów za pomocą modeli wizji AI",
    "modules.ai.vision.analyze.label": "Analiza Wizji",
    "modules.ai.vision.analyze.output.analysis.description": "Analiza AI obrazu",
    "modules.ai.vision.analyze.output.model.description": "Model używany do analizy",
    "modules.ai.vision.analyze.output.provider.description": "Dostawca używany do analizy",
    "modules.ai.vision.analyze.output.tokens_used.description": "Liczba użytych tokenów",
    "modules.ai.vision.analyze.params.api_key.description": "Klucz API (domyślnie zmienna środowiskowa)",
    "modules.ai.vision.analyze.params.api_key.label": "Klucz API",
    "modules.ai.vision.analyze.params.detail.description": "Poziom szczegółowości obrazu (niski/wysoki/auto)",
    "modules.ai.vision.analyze.params.detail.label": "Szczegóły",
    "modules.ai.vision.analyze.params.image_path.description": "Lokalna ścieżka do pliku obrazu",
    "modules.ai.vision.analyze.params.image_path.label": "Ścieżka obrazu",
    "modules.ai.vision.analyze.params.image_url.description": "URL obrazu do analizy",
    "modules.ai.vision.analyze.params.image_url.label": "URL obrazu",
    "modules.ai.vision.analyze.params.max_tokens.description": "Maksymalna liczba tokenów w odpowiedzi",
    "modules.ai.vision.analyze.params.max_tokens.label": "Maksymalna liczba tokenów",
    "modules.ai.vision.analyze.params.model.description": "Model wizji do użycia",
    "modules.ai.vision.analyze.params.model.label": "Model",
    "modules.ai.vision.analyze.params.prompt.description": "Co analizować lub pytać o obraz",
    "modules.ai.vision.analyze.params.prompt.label": "Polecenie",
    "modules.ai.vision.analyze.params.provider.description": "Dostawca AI do analizy wizji",
    "modules.ai.vision.analyze.params.provider.label": "Dostawca"
  }
}

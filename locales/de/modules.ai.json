{
  "$schema": "../../schema/locale.schema.json",
  "locale": "de",
  "category": "ai",
  "version": "1.0.0",
  "translations": {
    "modules.ai.embed.description": "Erstellen Sie Vektoreinbettungen aus Text mit KI-Modellen",
    "modules.ai.embed.label": "Texteinbettungen",
    "modules.ai.embed.output.dimensions.description": "Anzahl der Dimensionen im Einbettungsvektor",
    "modules.ai.embed.output.embeddings.description": "Vektor-Einbettungsarray",
    "modules.ai.embed.output.model.description": "Modell, das für die Einbettung verwendet wird",
    "modules.ai.embed.output.token_count.description": "Anzahl der verarbeiteten Tokens",
    "modules.ai.embed.params.api_key.description": "API-Schlüssel (fällt auf Umgebungsvariable zurück)",
    "modules.ai.embed.params.api_key.label": "API-Schlüssel",
    "modules.ai.embed.params.dimensions.description": "Einbettungsdimensionen (für Modelle, die dies unterstützen)",
    "modules.ai.embed.params.dimensions.label": "Dimensionen",
    "modules.ai.embed.params.model.description": "Einbettungsmodell zur Verwendung",
    "modules.ai.embed.params.model.label": "Modell",
    "modules.ai.embed.params.provider.description": "KI-Anbieter für Einbettungen",
    "modules.ai.embed.params.provider.label": "Anbieter",
    "modules.ai.embed.params.text.description": "Text zur Einbettung",
    "modules.ai.embed.params.text.label": "Text",
    "modules.ai.extract.description": "Strukturierte Daten aus Text mit KI extrahieren",
    "modules.ai.extract.label": "KI-Extraktion",
    "modules.ai.extract.output.extracted.description": "Extrahierte strukturierte Daten",
    "modules.ai.extract.output.model.description": "Modell, das für die Extraktion verwendet wird",
    "modules.ai.extract.output.raw_response.description": "Rohantwort des Modells",
    "modules.ai.extract.params.api_key.description": "API-Schlüssel (fällt auf Umgebungsvariable zurück)",
    "modules.ai.extract.params.api_key.label": "API-Schlüssel",
    "modules.ai.extract.params.instructions.description": "Zusätzliche Extraktionsanweisungen",
    "modules.ai.extract.params.instructions.label": "Anweisungen",
    "modules.ai.extract.params.model.description": "Modell zur Verwendung für die Extraktion",
    "modules.ai.extract.params.model.label": "Modell",
    "modules.ai.extract.params.provider.description": "Zu verwendender KI-Anbieter",
    "modules.ai.extract.params.provider.label": "Anbieter",
    "modules.ai.extract.params.schema.description": "JSON-Schema, das die zu extrahierenden Felder definiert",
    "modules.ai.extract.params.schema.label": "Schema",
    "modules.ai.extract.params.temperature.description": "Sampling-Temperatur (0-2)",
    "modules.ai.extract.params.temperature.label": "Temperatur",
    "modules.ai.extract.params.text.description": "Text, aus dem Daten extrahiert werden sollen",
    "modules.ai.extract.params.text.label": "Text",
    "modules.ai.local_ollama.chat.description": "Chat mit lokalem LLM über Ollama (komplett offline)",
    "modules.ai.local_ollama.chat.label": "Lokaler Ollama-Chat",
    "modules.ai.local_ollama.chat.output.context.description": "Antwort von der Operation",
    "modules.ai.local_ollama.chat.output.eval_count.description": "Modellladezeit",
    "modules.ai.local_ollama.chat.output.load_duration.description": "Konversationskontext für Folgeanfragen",
    "modules.ai.local_ollama.chat.output.model.description": "Antwort von der Operation",
    "modules.ai.local_ollama.chat.output.prompt_eval_count.description": "Gesamtverarbeitungsdauer",
    "modules.ai.local_ollama.chat.output.response.description": "Maximale Token in der Antwort (optional, modellabhängig)",
    "modules.ai.local_ollama.chat.output.total_duration.description": "Modellname oder -kennung",
    "modules.ai.local_ollama.chat.params.max_tokens.description": "Ollama-Server-URL",
    "modules.ai.local_ollama.chat.params.max_tokens.label": "Ollama-URL",
    "modules.ai.local_ollama.chat.params.model.description": "Die Nachricht, die an das lokale LLM gesendet werden soll",
    "modules.ai.local_ollama.chat.params.model.label": "Eingabe",
    "modules.ai.local_ollama.chat.params.ollama_url.description": "System-Rollennachricht (optional)",
    "modules.ai.local_ollama.chat.params.ollama_url.label": "Systemnachricht",
    "modules.ai.local_ollama.chat.params.prompt.description": "Die Nachricht, die an das lokale LLM gesendet werden soll",
    "modules.ai.local_ollama.chat.params.prompt.label": "Eingabe",
    "modules.ai.local_ollama.chat.params.system_message.description": "System-Rollennachricht (optional)",
    "modules.ai.local_ollama.chat.params.system_message.label": "Temperatur",
    "modules.ai.local_ollama.chat.params.temperature.description": "Sampling-Temperatur (0-2)",
    "modules.ai.local_ollama.chat.params.temperature.label": "CodeLlama 7B",
    "modules.ai.memory.description": "Konversationsgedächtnis für KI-Agent",
    "modules.ai.memory.entity.description": "Entitäten (Personen, Orte, Konzepte) aus Gesprächen extrahieren und verfolgen",
    "modules.ai.memory.entity.label": "Entitätsgedächtnis",
    "modules.ai.memory.entity.output.config.description": "Verfolgte Entitäten nach Typ",
    "modules.ai.memory.entity.output.entities.description": "Gedächtnistyp (Entität)",
    "modules.ai.memory.entity.output.memory_type.description": "Maximale Anzahl zu merkender Entitäten",
    "modules.ai.memory.entity.output.relationships.description": "Sitzungskennung",
    "modules.ai.memory.entity.output.session_id.description": "Maximale Anzahl zu merkender Entitäten",
    "modules.ai.memory.entity.params.entity_types": "Entitätstypen",
    "modules.ai.memory.entity.params.entity_types.options.concept": "Konzept",
    "modules.ai.memory.entity.params.entity_types.options.location": "Ort",
    "modules.ai.memory.entity.params.entity_types.options.organization": "Organisation",
    "modules.ai.memory.entity.params.entity_types.options.person": "Person",
    "modules.ai.memory.entity.params.extraction_model": "Extraktionsmodell",
    "modules.ai.memory.entity.params.max_entities": "Beziehungen verfolgen",
    "modules.ai.memory.entity.params.session_id": "Sitzungs-ID",
    "modules.ai.memory.entity.params.track_relationships": "Sitzungs-ID",
    "modules.ai.memory.entity.ports.memory": "Entitätstypen",
    "modules.ai.memory.label": "KI-Gedächtnis",
    "modules.ai.memory.options.buffer": "Gedächtnistyp",
    "modules.ai.memory.options.summary": "Fenstergröße",
    "modules.ai.memory.options.window": "Gedächtnistyp",
    "modules.ai.memory.output.config.description": "Sitzungskennung",
    "modules.ai.memory.output.memory_type.description": "Vorgeladener Konversationsverlauf",
    "modules.ai.memory.output.messages.description": "Gedächtnistyp",
    "modules.ai.memory.output.session_id.description": "Vorgeladener Konversationsverlauf",
    "modules.ai.memory.params.initial_messages": "Sitzungs-ID",
    "modules.ai.memory.params.initial_messages.description": "Vorgeladener Konversationsverlauf",
    "modules.ai.memory.params.memory_type": "Gedächtnistyp",
    "modules.ai.memory.params.memory_type.description": "Art der Gedächtnisspeicherung",
    "modules.ai.memory.params.session_id": "Sitzungs-ID",
    "modules.ai.memory.params.session_id.description": "Eindeutige Kennung für diese Konversationssitzung",
    "modules.ai.memory.params.window_size": "Fenstergröße",
    "modules.ai.memory.params.window_size.description": "Anzahl der beizubehaltenden letzten Nachrichten (für Fenstergedächtnis)",
    "modules.ai.memory.ports.memory": "Gedächtnistyp",
    "modules.ai.memory.redis.description": "Persistentes Konversationsgedächtnis mit Redis-Speicher",
    "modules.ai.memory.redis.label": "Redis-Gedächtnis",
    "modules.ai.memory.redis.output.config.description": "Geladener Nachrichtenverlauf",
    "modules.ai.memory.redis.output.connected.description": "Sitzungskennung",
    "modules.ai.memory.redis.output.memory_type.description": "Vorhandene Nachrichten aus Redis bei Initialisierung laden",
    "modules.ai.memory.redis.output.messages.description": "Gedächtnistyp (redis)",
    "modules.ai.memory.redis.output.session_id.description": "Vorhandene Nachrichten aus Redis bei Initialisierung laden",
    "modules.ai.memory.redis.params.key_prefix": "Redis-URL",
    "modules.ai.memory.redis.params.load_on_start": "Max. Nachrichten",
    "modules.ai.memory.redis.params.max_messages": "TTL (Sekunden)",
    "modules.ai.memory.redis.params.redis_url": "Redis-URL",
    "modules.ai.memory.redis.params.session_id": "Schlüsselpräfix",
    "modules.ai.memory.redis.params.ttl_seconds": "Sitzungs-ID",
    "modules.ai.memory.redis.ports.memory": "Redis-URL",
    "modules.ai.memory.vector.description": "Semantisches Gedächtnis mit Vektor-Embeddings für relevanten Kontextabruf",
    "modules.ai.memory.vector.label": "Vektor-Gedächtnis",
    "modules.ai.memory.vector.output.config.description": "Sitzungskennung",
    "modules.ai.memory.vector.output.embedding_model.description": "Gedächtnistyp (Vektor)",
    "modules.ai.memory.vector.output.memory_type.description": "Zeitstempel und andere Metadaten mit Erinnerungen einschließen",
    "modules.ai.memory.vector.output.session_id.description": "Zeitstempel und andere Metadaten mit Erinnerungen einschließen",
    "modules.ai.memory.vector.params.embedding_model": "Embedding-Modell",
    "modules.ai.memory.vector.params.include_metadata": "Sitzungs-ID",
    "modules.ai.memory.vector.params.session_id": "Ähnlichkeitsschwelle",
    "modules.ai.memory.vector.params.similarity_threshold": "Top-K-Ergebnisse",
    "modules.ai.memory.vector.params.top_k": "Top-K-Ergebnisse",
    "modules.ai.memory.vector.ports.memory": "Embedding-Modell",
    "modules.ai.model.description": "LLM-Modellkonfiguration für KI-Agent",
    "modules.ai.model.label": "KI-Modell",
    "modules.ai.model.output.config.description": "LLM-Provider-Name",
    "modules.ai.model.output.model.description": "LLM-Provider-Name",
    "modules.ai.model.output.provider.description": "Maximale Token in der Antwort",
    "modules.ai.model.params.max_tokens": "Max. Token",
    "modules.ai.model.params.max_tokens.description": "Maximale Token in der Antwort",
    "modules.ai.model.ports.model": "Modell",
    "modules.ai.vision.analyze.description": "Bilder mit KI-Vision-Modellen analysieren",
    "modules.ai.vision.analyze.label": "Vision-Analyse",
    "modules.ai.vision.analyze.output.analysis.description": "KI-Analyse des Bildes",
    "modules.ai.vision.analyze.output.model.description": "Modell, das für die Analyse verwendet wird",
    "modules.ai.vision.analyze.output.provider.description": "Anbieter, der für die Analyse verwendet wird",
    "modules.ai.vision.analyze.output.tokens_used.description": "Anzahl der verwendeten Tokens",
    "modules.ai.vision.analyze.params.api_key.description": "API-Schlüssel (fällt auf Umgebungsvariable zurück)",
    "modules.ai.vision.analyze.params.api_key.label": "API-Schlüssel",
    "modules.ai.vision.analyze.params.detail.description": "Bilddetailgrad (niedrig/hoch/auto)",
    "modules.ai.vision.analyze.params.detail.label": "Detail",
    "modules.ai.vision.analyze.params.image_path.description": "Lokaler Pfad zur Bilddatei",
    "modules.ai.vision.analyze.params.image_path.label": "Bildpfad",
    "modules.ai.vision.analyze.params.image_url.description": "URL des zu analysierenden Bildes",
    "modules.ai.vision.analyze.params.image_url.label": "Bild-URL",
    "modules.ai.vision.analyze.params.max_tokens.description": "Maximale Tokens in der Antwort",
    "modules.ai.vision.analyze.params.max_tokens.label": "Maximale Tokens",
    "modules.ai.vision.analyze.params.model.description": "Zu verwendendes Vision-Modell",
    "modules.ai.vision.analyze.params.model.label": "Modell",
    "modules.ai.vision.analyze.params.prompt.description": "Was soll analysiert oder zum Bild gefragt werden",
    "modules.ai.vision.analyze.params.prompt.label": "Eingabe",
    "modules.ai.vision.analyze.params.provider.description": "AI-Anbieter für die Bildanalyse",
    "modules.ai.vision.analyze.params.provider.label": "Anbieter"
  }
}

{
  "$schema": "../../schema/locale.schema.json",
  "locale": "es",
  "category": "ai",
  "version": "1.0.0",
  "translations": {
    "modules.ai.local_ollama.chat.description": "Chatear con LLM local via Ollama (completamente sin conexion)",
    "modules.ai.local_ollama.chat.label": "Chat local Ollama",
    "modules.ai.local_ollama.chat.output.context.description": "Respuesta de la operacion",
    "modules.ai.local_ollama.chat.output.eval_count.description": "Duracion de carga del modelo",
    "modules.ai.local_ollama.chat.output.load_duration.description": "Contexto de conversacion para solicitudes de seguimiento",
    "modules.ai.local_ollama.chat.output.model.description": "Respuesta de la operacion",
    "modules.ai.local_ollama.chat.output.prompt_eval_count.description": "Duracion total de procesamiento",
    "modules.ai.local_ollama.chat.output.response.description": "Maximo de tokens en respuesta (opcional, depende del modelo)",
    "modules.ai.local_ollama.chat.output.total_duration.description": "Nombre o identificador del modelo",
    "modules.ai.local_ollama.chat.params.max_tokens.description": "URL del servidor Ollama",
    "modules.ai.local_ollama.chat.params.max_tokens.label": "URL de Ollama",
    "modules.ai.local_ollama.chat.params.model.description": "El mensaje a enviar al LLM local",
    "modules.ai.local_ollama.chat.params.model.label": "Prompt",
    "modules.ai.local_ollama.chat.params.ollama_url.description": "Mensaje de rol del sistema (opcional)",
    "modules.ai.local_ollama.chat.params.ollama_url.label": "Mensaje del sistema",
    "modules.ai.local_ollama.chat.params.prompt.description": "El mensaje a enviar al LLM local",
    "modules.ai.local_ollama.chat.params.prompt.label": "Prompt",
    "modules.ai.local_ollama.chat.params.system_message.description": "Mensaje de rol del sistema (opcional)",
    "modules.ai.local_ollama.chat.params.system_message.label": "Temperatura",
    "modules.ai.local_ollama.chat.params.temperature.description": "Temperatura de muestreo (0-2)",
    "modules.ai.local_ollama.chat.params.temperature.label": "CodeLlama 7B",
    "modules.ai.memory.description": "Memoria de conversacion para agente de IA",
    "modules.ai.memory.entity.description": "Extraer y rastrear entidades (personas, lugares, conceptos) de conversaciones",
    "modules.ai.memory.entity.label": "Memoria de entidades",
    "modules.ai.memory.entity.output.config.description": "Entidades rastreadas por tipo",
    "modules.ai.memory.entity.output.entities.description": "Tipo de memoria (entidad)",
    "modules.ai.memory.entity.output.memory_type.description": "Maximo numero de entidades a recordar",
    "modules.ai.memory.entity.output.relationships.description": "Identificador de sesion",
    "modules.ai.memory.entity.output.session_id.description": "Maximo numero de entidades a recordar",
    "modules.ai.memory.entity.params.entity_types": "Tipos de entidad",
    "modules.ai.memory.entity.params.entity_types.options.concept": "Concept",
    "modules.ai.memory.entity.params.entity_types.options.location": "Location",
    "modules.ai.memory.entity.params.entity_types.options.organization": "Organization",
    "modules.ai.memory.entity.params.entity_types.options.person": "Person",
    "modules.ai.memory.entity.params.extraction_model": "Modelo de extraccion",
    "modules.ai.memory.entity.params.max_entities": "Rastrear relaciones",
    "modules.ai.memory.entity.params.session_id": "ID de sesion",
    "modules.ai.memory.entity.params.track_relationships": "ID de sesion",
    "modules.ai.memory.entity.ports.memory": "Tipos de entidad",
    "modules.ai.memory.label": "Memoria de IA",
    "modules.ai.memory.options.buffer": "Tipo de memoria",
    "modules.ai.memory.options.summary": "Tamano de ventana",
    "modules.ai.memory.options.window": "Tipo de memoria",
    "modules.ai.memory.output.config.description": "Identificador de sesion",
    "modules.ai.memory.output.memory_type.description": "Historial de conversacion precargado",
    "modules.ai.memory.output.messages.description": "Tipo de memoria",
    "modules.ai.memory.output.session_id.description": "Historial de conversacion precargado",
    "modules.ai.memory.params.initial_messages": "ID de sesion",
    "modules.ai.memory.params.initial_messages.description": "Historial de conversacion precargado",
    "modules.ai.memory.params.memory_type": "Tipo de memoria",
    "modules.ai.memory.params.memory_type.description": "Tipo de almacenamiento de memoria",
    "modules.ai.memory.params.session_id": "ID de sesion",
    "modules.ai.memory.params.session_id.description": "Identificador unico para esta sesion de conversacion",
    "modules.ai.memory.params.window_size": "Tamano de ventana",
    "modules.ai.memory.params.window_size.description": "Numero de mensajes recientes a mantener (para memoria de ventana)",
    "modules.ai.memory.ports.memory": "Tipo de memoria",
    "modules.ai.memory.redis.description": "Memoria de conversacion persistente usando almacenamiento Redis",
    "modules.ai.memory.redis.label": "Memoria Redis",
    "modules.ai.memory.redis.output.config.description": "Historial de mensajes cargado",
    "modules.ai.memory.redis.output.connected.description": "Identificador de sesion",
    "modules.ai.memory.redis.output.memory_type.description": "Cargar mensajes existentes de Redis en inicializacion",
    "modules.ai.memory.redis.output.messages.description": "Tipo de memoria (redis)",
    "modules.ai.memory.redis.output.session_id.description": "Cargar mensajes existentes de Redis en inicializacion",
    "modules.ai.memory.redis.params.key_prefix": "URL de Redis",
    "modules.ai.memory.redis.params.load_on_start": "Maximo de mensajes",
    "modules.ai.memory.redis.params.max_messages": "TTL (segundos)",
    "modules.ai.memory.redis.params.redis_url": "URL de Redis",
    "modules.ai.memory.redis.params.session_id": "Prefijo de clave",
    "modules.ai.memory.redis.params.ttl_seconds": "ID de sesion",
    "modules.ai.memory.redis.ports.memory": "URL de Redis",
    "modules.ai.memory.vector.description": "Memoria semantica usando embeddings vectoriales para recuperacion de contexto relevante",
    "modules.ai.memory.vector.label": "Memoria vectorial",
    "modules.ai.memory.vector.output.config.description": "Identificador de sesion",
    "modules.ai.memory.vector.output.embedding_model.description": "Tipo de memoria (vector)",
    "modules.ai.memory.vector.output.memory_type.description": "Incluir marca de tiempo y otros metadatos con memorias",
    "modules.ai.memory.vector.output.session_id.description": "Incluir marca de tiempo y otros metadatos con memorias",
    "modules.ai.memory.vector.params.embedding_model": "Modelo de embedding",
    "modules.ai.memory.vector.params.include_metadata": "ID de sesion",
    "modules.ai.memory.vector.params.session_id": "Umbral de similitud",
    "modules.ai.memory.vector.params.similarity_threshold": "Top K resultados",
    "modules.ai.memory.vector.params.top_k": "Top K resultados",
    "modules.ai.memory.vector.ports.memory": "Modelo de embedding",
    "modules.ai.model.description": "Configuracion de modelo LLM para agente de IA",
    "modules.ai.model.label": "Modelo de IA",
    "modules.ai.model.output.config.description": "Nombre del proveedor de LLM",
    "modules.ai.model.output.model.description": "Nombre del proveedor de LLM",
    "modules.ai.model.output.provider.description": "Maximo de tokens en respuesta",
    "modules.ai.model.params.max_tokens": "Maximo de tokens",
    "modules.ai.model.params.max_tokens.description": "Maximo de tokens en respuesta",
    "modules.ai.model.ports.model": "Modelo"
  }
}

{
  "$schema": "../../schema/locale.schema.json",
  "locale": "en",
  "category": "ai",
  "version": "1.0.0",
  "translations": {
    "modules.ai.local_ollama.chat.description": "Chat with local LLM via Ollama (completely offline)",
    "modules.ai.local_ollama.chat.label": "Local Ollama Chat",
    "modules.ai.local_ollama.chat.output.context.description": "Response from the operation",
    "modules.ai.local_ollama.chat.output.eval_count.description": "Model loading duration",
    "modules.ai.local_ollama.chat.output.load_duration.description": "Conversation context for follow-up requests",
    "modules.ai.local_ollama.chat.output.model.description": "Response from the operation",
    "modules.ai.local_ollama.chat.output.prompt_eval_count.description": "Total processing duration",
    "modules.ai.local_ollama.chat.output.response.description": "Maximum tokens in response (optional, depends on model)",
    "modules.ai.local_ollama.chat.output.total_duration.description": "Model name or identifier",
    "modules.ai.local_ollama.chat.params.max_tokens.description": "Ollama server URL",
    "modules.ai.local_ollama.chat.params.max_tokens.label": "Ollama URL",
    "modules.ai.local_ollama.chat.params.model.description": "The message to send to the local LLM",
    "modules.ai.local_ollama.chat.params.model.label": "Prompt",
    "modules.ai.local_ollama.chat.params.ollama_url.description": "System role message (optional)",
    "modules.ai.local_ollama.chat.params.ollama_url.label": "System Message",
    "modules.ai.local_ollama.chat.params.prompt.description": "The message to send to the local LLM",
    "modules.ai.local_ollama.chat.params.prompt.label": "Prompt",
    "modules.ai.local_ollama.chat.params.system_message.description": "System role message (optional)",
    "modules.ai.local_ollama.chat.params.system_message.label": "Temperature",
    "modules.ai.local_ollama.chat.params.temperature.description": "Sampling temperature (0-2)",
    "modules.ai.local_ollama.chat.params.temperature.label": "CodeLlama 7B",
    "modules.ai.memory.description": "Conversation memory for AI Agent",
    "modules.ai.memory.entity.description": "Extract and track entities (people, places, concepts) from conversations",
    "modules.ai.memory.entity.label": "Entity Memory",
    "modules.ai.memory.entity.output.config.description": "Tracked entities by type",
    "modules.ai.memory.entity.output.entities.description": "Type of memory (entity)",
    "modules.ai.memory.entity.output.memory_type.description": "Maximum number of entities to remember",
    "modules.ai.memory.entity.output.relationships.description": "Session identifier",
    "modules.ai.memory.entity.output.session_id.description": "Maximum number of entities to remember",
    "modules.ai.memory.entity.params.entity_types": "Entity Types",
    "modules.ai.memory.entity.params.entity_types.options.concept": "Concept",
    "modules.ai.memory.entity.params.entity_types.options.location": "Location",
    "modules.ai.memory.entity.params.entity_types.options.organization": "Organization",
    "modules.ai.memory.entity.params.entity_types.options.person": "Person",
    "modules.ai.memory.entity.params.extraction_model": "Extraction Model",
    "modules.ai.memory.entity.params.max_entities": "Track Relationships",
    "modules.ai.memory.entity.params.session_id": "Session ID",
    "modules.ai.memory.entity.params.track_relationships": "Session ID",
    "modules.ai.memory.entity.ports.memory": "Entity Types",
    "modules.ai.memory.label": "AI Memory",
    "modules.ai.memory.options.buffer": "Memory Type",
    "modules.ai.memory.options.summary": "Window Size",
    "modules.ai.memory.options.window": "Memory Type",
    "modules.ai.memory.output.config.description": "Session identifier",
    "modules.ai.memory.output.memory_type.description": "Pre-loaded conversation history",
    "modules.ai.memory.output.messages.description": "Type of memory",
    "modules.ai.memory.output.session_id.description": "Pre-loaded conversation history",
    "modules.ai.memory.params.initial_messages": "Session ID",
    "modules.ai.memory.params.initial_messages.description": "Pre-loaded conversation history",
    "modules.ai.memory.params.memory_type": "Memory Type",
    "modules.ai.memory.params.memory_type.description": "Type of memory storage",
    "modules.ai.memory.params.session_id": "Session ID",
    "modules.ai.memory.params.session_id.description": "Unique identifier for this conversation session",
    "modules.ai.memory.params.window_size": "Window Size",
    "modules.ai.memory.params.window_size.description": "Number of recent messages to keep (for window memory)",
    "modules.ai.memory.ports.memory": "Memory Type",
    "modules.ai.memory.redis.description": "Persistent conversation memory using Redis storage",
    "modules.ai.memory.redis.label": "Redis Memory",
    "modules.ai.memory.redis.output.config.description": "Loaded message history",
    "modules.ai.memory.redis.output.connected.description": "Session identifier",
    "modules.ai.memory.redis.output.memory_type.description": "Load existing messages from Redis on initialization",
    "modules.ai.memory.redis.output.messages.description": "Type of memory (redis)",
    "modules.ai.memory.redis.output.session_id.description": "Load existing messages from Redis on initialization",
    "modules.ai.memory.redis.params.key_prefix": "Redis URL",
    "modules.ai.memory.redis.params.load_on_start": "Max Messages",
    "modules.ai.memory.redis.params.max_messages": "TTL (seconds)",
    "modules.ai.memory.redis.params.redis_url": "Redis URL",
    "modules.ai.memory.redis.params.session_id": "Key Prefix",
    "modules.ai.memory.redis.params.ttl_seconds": "Session ID",
    "modules.ai.memory.redis.ports.memory": "Redis URL",
    "modules.ai.memory.vector.description": "Semantic memory using vector embeddings for relevant context retrieval",
    "modules.ai.memory.vector.label": "Vector Memory",
    "modules.ai.memory.vector.output.config.description": "Session identifier",
    "modules.ai.memory.vector.output.embedding_model.description": "Type of memory (vector)",
    "modules.ai.memory.vector.output.memory_type.description": "Include timestamp and other metadata with memories",
    "modules.ai.memory.vector.output.session_id.description": "Include timestamp and other metadata with memories",
    "modules.ai.memory.vector.params.embedding_model": "Embedding Model",
    "modules.ai.memory.vector.params.include_metadata": "Session ID",
    "modules.ai.memory.vector.params.session_id": "Similarity Threshold",
    "modules.ai.memory.vector.params.similarity_threshold": "Top K Results",
    "modules.ai.memory.vector.params.top_k": "Top K Results",
    "modules.ai.memory.vector.ports.memory": "Embedding Model",
    "modules.ai.model.description": "LLM model configuration for AI Agent",
    "modules.ai.model.label": "AI Model",
    "modules.ai.model.output.config.description": "LLM provider name",
    "modules.ai.model.output.model.description": "LLM provider name",
    "modules.ai.model.output.provider.description": "Maximum tokens in response",
    "modules.ai.model.params.max_tokens": "Max Tokens",
    "modules.ai.model.params.max_tokens.description": "Maximum tokens in response",
    "modules.ai.model.ports.model": "Model",
    "modules.ai.embed.description": "Generate vector embeddings from text using AI models",
    "modules.ai.embed.label": "Text Embeddings",
    "modules.ai.embed.output.dimensions.description": "Number of dimensions in the embedding vector",
    "modules.ai.embed.output.embeddings.description": "Vector embedding array",
    "modules.ai.embed.output.model.description": "Model used for embedding",
    "modules.ai.embed.output.token_count.description": "Number of tokens processed",
    "modules.ai.embed.params.api_key.description": "API key (falls back to environment variable)",
    "modules.ai.embed.params.api_key.label": "API Key",
    "modules.ai.embed.params.dimensions.description": "Embedding dimensions (for models that support it)",
    "modules.ai.embed.params.dimensions.label": "Dimensions",
    "modules.ai.embed.params.model.description": "Embedding model to use",
    "modules.ai.embed.params.model.label": "Model",
    "modules.ai.embed.params.provider.description": "AI provider for embeddings",
    "modules.ai.embed.params.provider.label": "Provider",
    "modules.ai.embed.params.text.description": "Text to embed",
    "modules.ai.embed.params.text.label": "Text",
    "modules.ai.extract.description": "Extract structured data from text using AI",
    "modules.ai.extract.label": "AI Extract",
    "modules.ai.extract.output.extracted.description": "Extracted structured data",
    "modules.ai.extract.output.model.description": "Model used for extraction",
    "modules.ai.extract.output.raw_response.description": "Raw model response",
    "modules.ai.extract.params.api_key.description": "API key (falls back to environment variable)",
    "modules.ai.extract.params.api_key.label": "API Key",
    "modules.ai.extract.params.instructions.description": "Additional extraction instructions",
    "modules.ai.extract.params.instructions.label": "Instructions",
    "modules.ai.extract.params.model.description": "Model to use for extraction",
    "modules.ai.extract.params.model.label": "Model",
    "modules.ai.extract.params.provider.description": "AI provider to use",
    "modules.ai.extract.params.provider.label": "Provider",
    "modules.ai.extract.params.schema.description": "JSON schema defining the fields to extract",
    "modules.ai.extract.params.schema.label": "Schema",
    "modules.ai.extract.params.temperature.description": "Sampling temperature (0-2)",
    "modules.ai.extract.params.temperature.label": "Temperature",
    "modules.ai.extract.params.text.description": "Text to extract data from",
    "modules.ai.extract.params.text.label": "Text",
    "modules.ai.vision.analyze.description": "Analyze images using AI vision models",
    "modules.ai.vision.analyze.label": "Vision Analyze",
    "modules.ai.vision.analyze.output.analysis.description": "AI analysis of the image",
    "modules.ai.vision.analyze.output.model.description": "Model used for analysis",
    "modules.ai.vision.analyze.output.provider.description": "Provider used for analysis",
    "modules.ai.vision.analyze.output.tokens_used.description": "Number of tokens used",
    "modules.ai.vision.analyze.params.api_key.description": "API key (falls back to environment variable)",
    "modules.ai.vision.analyze.params.api_key.label": "API Key",
    "modules.ai.vision.analyze.params.detail.description": "Image detail level (low/high/auto)",
    "modules.ai.vision.analyze.params.detail.label": "Detail",
    "modules.ai.vision.analyze.params.image_path.description": "Local path to image file",
    "modules.ai.vision.analyze.params.image_path.label": "Image Path",
    "modules.ai.vision.analyze.params.image_url.description": "URL of the image to analyze",
    "modules.ai.vision.analyze.params.image_url.label": "Image URL",
    "modules.ai.vision.analyze.params.max_tokens.description": "Maximum tokens in response",
    "modules.ai.vision.analyze.params.max_tokens.label": "Max Tokens",
    "modules.ai.vision.analyze.params.model.description": "Vision model to use",
    "modules.ai.vision.analyze.params.model.label": "Model",
    "modules.ai.vision.analyze.params.prompt.description": "What to analyze or ask about the image",
    "modules.ai.vision.analyze.params.prompt.label": "Prompt",
    "modules.ai.vision.analyze.params.provider.description": "AI provider for vision analysis",
    "modules.ai.vision.analyze.params.provider.label": "Provider"
  }
}
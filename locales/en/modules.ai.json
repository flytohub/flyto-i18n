{
  "$schema": "../../schema/locale.schema.json",
  "locale": "en",
  "category": "ai",
  "version": "1.0.0",
  "translations": {
    "modules.ai.local_ollama.chat.description": "Chat with local LLM via Ollama (completely offline)",
    "modules.ai.local_ollama.chat.label": "Local Ollama Chat",
    "modules.ai.local_ollama.chat.output.context.description": "Response from the operation",
    "modules.ai.local_ollama.chat.output.eval_count.description": "Model loading duration",
    "modules.ai.local_ollama.chat.output.load_duration.description": "Conversation context for follow-up requests",
    "modules.ai.local_ollama.chat.output.model.description": "Response from the operation",
    "modules.ai.local_ollama.chat.output.prompt_eval_count.description": "Total processing duration",
    "modules.ai.local_ollama.chat.output.response.description": "Maximum tokens in response (optional, depends on model)",
    "modules.ai.local_ollama.chat.output.total_duration.description": "Model name or identifier",
    "modules.ai.local_ollama.chat.params.max_tokens.description": "Ollama server URL",
    "modules.ai.local_ollama.chat.params.max_tokens.label": "Ollama URL",
    "modules.ai.local_ollama.chat.params.model.description": "The message to send to the local LLM",
    "modules.ai.local_ollama.chat.params.model.label": "Prompt",
    "modules.ai.local_ollama.chat.params.ollama_url.description": "System role message (optional)",
    "modules.ai.local_ollama.chat.params.ollama_url.label": "System Message",
    "modules.ai.local_ollama.chat.params.prompt.description": "The message to send to the local LLM",
    "modules.ai.local_ollama.chat.params.prompt.label": "Prompt",
    "modules.ai.local_ollama.chat.params.system_message.description": "System role message (optional)",
    "modules.ai.local_ollama.chat.params.system_message.label": "Temperature",
    "modules.ai.local_ollama.chat.params.temperature.description": "Sampling temperature (0-2)",
    "modules.ai.local_ollama.chat.params.temperature.label": "CodeLlama 7B",
    "modules.ai.memory.description": "Conversation memory for AI Agent",
    "modules.ai.memory.entity.description": "Extract and track entities (people, places, concepts) from conversations",
    "modules.ai.memory.entity.label": "Entity Memory",
    "modules.ai.memory.entity.output.config.description": "Tracked entities by type",
    "modules.ai.memory.entity.output.entities.description": "Type of memory (entity)",
    "modules.ai.memory.entity.output.memory_type.description": "Maximum number of entities to remember",
    "modules.ai.memory.entity.output.relationships.description": "Session identifier",
    "modules.ai.memory.entity.output.session_id.description": "Maximum number of entities to remember",
    "modules.ai.memory.entity.params.entity_types": "Entity Types",
    "modules.ai.memory.entity.params.entity_types.options.concept": "Concept",
    "modules.ai.memory.entity.params.entity_types.options.location": "Location",
    "modules.ai.memory.entity.params.entity_types.options.organization": "Organization",
    "modules.ai.memory.entity.params.entity_types.options.person": "Person",
    "modules.ai.memory.entity.params.extraction_model": "Extraction Model",
    "modules.ai.memory.entity.params.max_entities": "Track Relationships",
    "modules.ai.memory.entity.params.session_id": "Session ID",
    "modules.ai.memory.entity.params.track_relationships": "Session ID",
    "modules.ai.memory.entity.ports.memory": "Entity Types",
    "modules.ai.memory.label": "AI Memory",
    "modules.ai.memory.options.buffer": "Memory Type",
    "modules.ai.memory.options.summary": "Window Size",
    "modules.ai.memory.options.window": "Memory Type",
    "modules.ai.memory.output.config.description": "Session identifier",
    "modules.ai.memory.output.memory_type.description": "Pre-loaded conversation history",
    "modules.ai.memory.output.messages.description": "Type of memory",
    "modules.ai.memory.output.session_id.description": "Pre-loaded conversation history",
    "modules.ai.memory.params.initial_messages": "Session ID",
    "modules.ai.memory.params.initial_messages.description": "Pre-loaded conversation history",
    "modules.ai.memory.params.memory_type": "Memory Type",
    "modules.ai.memory.params.memory_type.description": "Type of memory storage",
    "modules.ai.memory.params.session_id": "Session ID",
    "modules.ai.memory.params.session_id.description": "Unique identifier for this conversation session",
    "modules.ai.memory.params.window_size": "Window Size",
    "modules.ai.memory.params.window_size.description": "Number of recent messages to keep (for window memory)",
    "modules.ai.memory.ports.memory": "Memory Type",
    "modules.ai.memory.redis.description": "Persistent conversation memory using Redis storage",
    "modules.ai.memory.redis.label": "Redis Memory",
    "modules.ai.memory.redis.output.config.description": "Loaded message history",
    "modules.ai.memory.redis.output.connected.description": "Session identifier",
    "modules.ai.memory.redis.output.memory_type.description": "Load existing messages from Redis on initialization",
    "modules.ai.memory.redis.output.messages.description": "Type of memory (redis)",
    "modules.ai.memory.redis.output.session_id.description": "Load existing messages from Redis on initialization",
    "modules.ai.memory.redis.params.key_prefix": "Redis URL",
    "modules.ai.memory.redis.params.load_on_start": "Max Messages",
    "modules.ai.memory.redis.params.max_messages": "TTL (seconds)",
    "modules.ai.memory.redis.params.redis_url": "Redis URL",
    "modules.ai.memory.redis.params.session_id": "Key Prefix",
    "modules.ai.memory.redis.params.ttl_seconds": "Session ID",
    "modules.ai.memory.redis.ports.memory": "Redis URL",
    "modules.ai.memory.vector.description": "Semantic memory using vector embeddings for relevant context retrieval",
    "modules.ai.memory.vector.label": "Vector Memory",
    "modules.ai.memory.vector.output.config.description": "Session identifier",
    "modules.ai.memory.vector.output.embedding_model.description": "Type of memory (vector)",
    "modules.ai.memory.vector.output.memory_type.description": "Include timestamp and other metadata with memories",
    "modules.ai.memory.vector.output.session_id.description": "Include timestamp and other metadata with memories",
    "modules.ai.memory.vector.params.embedding_model": "Embedding Model",
    "modules.ai.memory.vector.params.include_metadata": "Session ID",
    "modules.ai.memory.vector.params.session_id": "Similarity Threshold",
    "modules.ai.memory.vector.params.similarity_threshold": "Top K Results",
    "modules.ai.memory.vector.params.top_k": "Top K Results",
    "modules.ai.memory.vector.ports.memory": "Embedding Model",
    "modules.ai.model.description": "LLM model configuration for AI Agent",
    "modules.ai.model.label": "AI Model",
    "modules.ai.model.output.config.description": "LLM provider name",
    "modules.ai.model.output.model.description": "LLM provider name",
    "modules.ai.model.output.provider.description": "Maximum tokens in response",
    "modules.ai.model.params.max_tokens": "Max Tokens",
    "modules.ai.model.params.max_tokens.description": "Maximum tokens in response",
    "modules.ai.model.ports.model": "Model"
  }
}
{
  "$schema": "../../schema/locale.schema.json",
  "locale": "it",
  "category": "ai",
  "version": "1.0.0",
  "translations": {
    "modules.ai.embed.description": "Genera incorporamenti vettoriali dal testo usando modelli AI",
    "modules.ai.embed.label": "Incorporamenti Testuali",
    "modules.ai.embed.output.dimensions.description": "Numero di dimensioni nel vettore di incorporamento",
    "modules.ai.embed.output.embeddings.description": "Array di incorporamenti vettoriali",
    "modules.ai.embed.output.model.description": "Modello usato per l'incorporamento",
    "modules.ai.embed.output.token_count.description": "Numero di token elaborati",
    "modules.ai.embed.params.api_key.description": "Chiave API (predefinito alla variabile d'ambiente)",
    "modules.ai.embed.params.api_key.label": "Chiave API",
    "modules.ai.embed.params.dimensions.description": "Dimensioni di incorporamento (per modelli che lo supportano)",
    "modules.ai.embed.params.dimensions.label": "Dimensioni",
    "modules.ai.embed.params.model.description": "Modello di incorporamento da usare",
    "modules.ai.embed.params.model.label": "Modello",
    "modules.ai.embed.params.provider.description": "Fornitore AI per incorporamenti",
    "modules.ai.embed.params.provider.label": "Fornitore",
    "modules.ai.embed.params.text.description": "Testo da incorporare",
    "modules.ai.embed.params.text.label": "Testo",
    "modules.ai.extract.description": "Estrai dati strutturati dal testo usando AI",
    "modules.ai.extract.label": "Estrazione AI",
    "modules.ai.extract.output.extracted.description": "Dati strutturati estratti",
    "modules.ai.extract.output.model.description": "Modello usato per l'estrazione",
    "modules.ai.extract.output.raw_response.description": "Risposta grezza del modello",
    "modules.ai.extract.params.api_key.description": "Chiave API (predefinito alla variabile d'ambiente)",
    "modules.ai.extract.params.api_key.label": "Chiave API",
    "modules.ai.extract.params.instructions.description": "Istruzioni aggiuntive per l'estrazione",
    "modules.ai.extract.params.instructions.label": "Istruzioni",
    "modules.ai.extract.params.model.description": "Modello da usare per l'estrazione",
    "modules.ai.extract.params.model.label": "Modello",
    "modules.ai.extract.params.provider.description": "Fornitore AI da usare",
    "modules.ai.extract.params.provider.label": "Fornitore",
    "modules.ai.extract.params.schema.description": "Schema JSON che definisce i campi da estrarre",
    "modules.ai.extract.params.schema.label": "Schema",
    "modules.ai.extract.params.temperature.description": "Temperatura di campionamento (0-2)",
    "modules.ai.extract.params.temperature.label": "Temperatura",
    "modules.ai.extract.params.text.description": "Testo da cui estrarre dati",
    "modules.ai.extract.params.text.label": "Testo",
    "modules.ai.local_ollama.chat.description": "Chatta con LLM locale tramite Ollama (completamente offline)",
    "modules.ai.local_ollama.chat.label": "Chat Ollama Locale",
    "modules.ai.local_ollama.chat.output.context.description": "Risposta dall'operazione",
    "modules.ai.local_ollama.chat.output.eval_count.description": "Durata caricamento modello",
    "modules.ai.local_ollama.chat.output.load_duration.description": "Contesto conversazione per richieste successive",
    "modules.ai.local_ollama.chat.output.model.description": "Risposta dall'operazione",
    "modules.ai.local_ollama.chat.output.prompt_eval_count.description": "Durata totale elaborazione",
    "modules.ai.local_ollama.chat.output.response.description": "Token massimi nella risposta (opzionale, dipende dal modello)",
    "modules.ai.local_ollama.chat.output.total_duration.description": "Nome o identificatore modello",
    "modules.ai.local_ollama.chat.params.max_tokens.description": "URL server Ollama",
    "modules.ai.local_ollama.chat.params.max_tokens.label": "URL Ollama",
    "modules.ai.local_ollama.chat.params.model.description": "Il messaggio da inviare al LLM locale",
    "modules.ai.local_ollama.chat.params.model.label": "Prompt",
    "modules.ai.local_ollama.chat.params.ollama_url.description": "Messaggio ruolo sistema (opzionale)",
    "modules.ai.local_ollama.chat.params.ollama_url.label": "Messaggio Sistema",
    "modules.ai.local_ollama.chat.params.prompt.description": "Il messaggio da inviare al LLM locale",
    "modules.ai.local_ollama.chat.params.prompt.label": "Prompt",
    "modules.ai.local_ollama.chat.params.system_message.description": "Messaggio ruolo sistema (opzionale)",
    "modules.ai.local_ollama.chat.params.system_message.label": "Temperatura",
    "modules.ai.local_ollama.chat.params.temperature.description": "Temperatura di campionamento (0-2)",
    "modules.ai.local_ollama.chat.params.temperature.label": "CodeLlama 7B",
    "modules.ai.memory.description": "Memoria conversazione per Agente AI",
    "modules.ai.memory.entity.description": "Estrai e traccia entita (persone, luoghi, concetti) dalle conversazioni",
    "modules.ai.memory.entity.label": "Memoria Entita",
    "modules.ai.memory.entity.output.config.description": "Entita tracciate per tipo",
    "modules.ai.memory.entity.output.entities.description": "Tipo di memoria (entita)",
    "modules.ai.memory.entity.output.memory_type.description": "Numero massimo di entita da ricordare",
    "modules.ai.memory.entity.output.relationships.description": "Identificatore sessione",
    "modules.ai.memory.entity.output.session_id.description": "Numero massimo di entita da ricordare",
    "modules.ai.memory.entity.params.entity_types": "Tipi Entita",
    "modules.ai.memory.entity.params.entity_types.options.concept": "Concept",
    "modules.ai.memory.entity.params.entity_types.options.location": "Location",
    "modules.ai.memory.entity.params.entity_types.options.organization": "Organization",
    "modules.ai.memory.entity.params.entity_types.options.person": "Person",
    "modules.ai.memory.entity.params.extraction_model": "Modello Estrazione",
    "modules.ai.memory.entity.params.max_entities": "Traccia Relazioni",
    "modules.ai.memory.entity.params.session_id": "ID Sessione",
    "modules.ai.memory.entity.params.track_relationships": "ID Sessione",
    "modules.ai.memory.entity.ports.memory": "Tipi Entita",
    "modules.ai.memory.label": "Memoria AI",
    "modules.ai.memory.options.buffer": "Tipo Memoria",
    "modules.ai.memory.options.summary": "Dimensione Finestra",
    "modules.ai.memory.options.window": "Tipo Memoria",
    "modules.ai.memory.output.config.description": "Identificatore sessione",
    "modules.ai.memory.output.memory_type.description": "Cronologia conversazione precaricata",
    "modules.ai.memory.output.messages.description": "Tipo di memoria",
    "modules.ai.memory.output.session_id.description": "Cronologia conversazione precaricata",
    "modules.ai.memory.params.initial_messages": "ID Sessione",
    "modules.ai.memory.params.initial_messages.description": "Cronologia conversazione precaricata",
    "modules.ai.memory.params.memory_type": "Tipo Memoria",
    "modules.ai.memory.params.memory_type.description": "Tipo di archiviazione memoria",
    "modules.ai.memory.params.session_id": "ID Sessione",
    "modules.ai.memory.params.session_id.description": "Identificatore univoco per questa sessione di conversazione",
    "modules.ai.memory.params.window_size": "Dimensione Finestra",
    "modules.ai.memory.params.window_size.description": "Numero di messaggi recenti da mantenere (per memoria finestra)",
    "modules.ai.memory.ports.memory": "Tipo Memoria",
    "modules.ai.memory.redis.description": "Memoria conversazione persistente usando archiviazione Redis",
    "modules.ai.memory.redis.label": "Memoria Redis",
    "modules.ai.memory.redis.output.config.description": "Cronologia messaggi caricata",
    "modules.ai.memory.redis.output.connected.description": "Identificatore sessione",
    "modules.ai.memory.redis.output.memory_type.description": "Carica messaggi esistenti da Redis all'inizializzazione",
    "modules.ai.memory.redis.output.messages.description": "Tipo di memoria (redis)",
    "modules.ai.memory.redis.output.session_id.description": "Carica messaggi esistenti da Redis all'inizializzazione",
    "modules.ai.memory.redis.params.key_prefix": "URL Redis",
    "modules.ai.memory.redis.params.load_on_start": "Messaggi Max",
    "modules.ai.memory.redis.params.max_messages": "TTL (secondi)",
    "modules.ai.memory.redis.params.redis_url": "URL Redis",
    "modules.ai.memory.redis.params.session_id": "Prefisso Chiave",
    "modules.ai.memory.redis.params.ttl_seconds": "ID Sessione",
    "modules.ai.memory.redis.ports.memory": "URL Redis",
    "modules.ai.memory.vector.description": "Memoria semantica usando embedding vettoriali per recupero contesto rilevante",
    "modules.ai.memory.vector.label": "Memoria Vettoriale",
    "modules.ai.memory.vector.output.config.description": "Identificatore sessione",
    "modules.ai.memory.vector.output.embedding_model.description": "Tipo di memoria (vettoriale)",
    "modules.ai.memory.vector.output.memory_type.description": "Includi timestamp e altri metadati con le memorie",
    "modules.ai.memory.vector.output.session_id.description": "Includi timestamp e altri metadati con le memorie",
    "modules.ai.memory.vector.params.embedding_model": "Modello Embedding",
    "modules.ai.memory.vector.params.include_metadata": "ID Sessione",
    "modules.ai.memory.vector.params.session_id": "Soglia Similarita",
    "modules.ai.memory.vector.params.similarity_threshold": "Top K Risultati",
    "modules.ai.memory.vector.params.top_k": "Top K Risultati",
    "modules.ai.memory.vector.ports.memory": "Modello Embedding",
    "modules.ai.model.description": "Configurazione modello LLM per Agente AI",
    "modules.ai.model.label": "Modello AI",
    "modules.ai.model.output.config.description": "Nome provider LLM",
    "modules.ai.model.output.model.description": "Nome provider LLM",
    "modules.ai.model.output.provider.description": "Token massimi nella risposta",
    "modules.ai.model.params.max_tokens": "Token Max",
    "modules.ai.model.params.max_tokens.description": "Token massimi nella risposta",
    "modules.ai.model.ports.model": "Modello",
    "modules.ai.vision.analyze.description": "Analizza immagini usando modelli di visione AI",
    "modules.ai.vision.analyze.label": "Analisi Visione",
    "modules.ai.vision.analyze.output.analysis.description": "Analisi AI dell'immagine",
    "modules.ai.vision.analyze.output.model.description": "Modello usato per l'analisi",
    "modules.ai.vision.analyze.output.provider.description": "Fornitore usato per l'analisi",
    "modules.ai.vision.analyze.output.tokens_used.description": "Numero di token usati",
    "modules.ai.vision.analyze.params.api_key.description": "Chiave API (predefinito alla variabile d'ambiente)",
    "modules.ai.vision.analyze.params.api_key.label": "Chiave API",
    "modules.ai.vision.analyze.params.detail.description": "Livello di dettaglio dell'immagine (basso/alto/auto)",
    "modules.ai.vision.analyze.params.detail.label": "Dettaglio",
    "modules.ai.vision.analyze.params.image_path.description": "Percorso locale al file immagine",
    "modules.ai.vision.analyze.params.image_path.label": "Percorso Immagine",
    "modules.ai.vision.analyze.params.image_url.description": "URL dell'immagine da analizzare",
    "modules.ai.vision.analyze.params.image_url.label": "URL Immagine",
    "modules.ai.vision.analyze.params.max_tokens.description": "Massimo numero di token nella risposta",
    "modules.ai.vision.analyze.params.max_tokens.label": "Token Massimi",
    "modules.ai.vision.analyze.params.model.description": "Modello di visione da usare",
    "modules.ai.vision.analyze.params.model.label": "Modello",
    "modules.ai.vision.analyze.params.prompt.description": "Cosa analizzare o chiedere sull'immagine",
    "modules.ai.vision.analyze.params.prompt.label": "Prompt",
    "modules.ai.vision.analyze.params.provider.description": "Fornitore AI per l'analisi visiva",
    "modules.ai.vision.analyze.params.provider.label": "Fornitore"
  }
}

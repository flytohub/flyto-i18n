{
  "$schema": "../../schema/locale.schema.json",
  "locale": "pt-BR",
  "category": "ai",
  "version": "1.0.0",
  "translations": {
    "modules.ai.embed.description": "Gere embeddings vetoriais de texto usando modelos de IA",
    "modules.ai.embed.label": "Embeddings de Texto",
    "modules.ai.embed.output.dimensions.description": "Número de dimensões no vetor de embedding",
    "modules.ai.embed.output.embeddings.description": "Array de embedding vetorial",
    "modules.ai.embed.output.model.description": "Modelo usado para embedding",
    "modules.ai.embed.output.token_count.description": "Número de tokens processados",
    "modules.ai.embed.params.api_key.description": "Chave de API (usa variável de ambiente como padrão)",
    "modules.ai.embed.params.api_key.label": "Chave de API",
    "modules.ai.embed.params.dimensions.description": "Dimensões de embedding (para modelos que suportam)",
    "modules.ai.embed.params.dimensions.label": "Dimensões",
    "modules.ai.embed.params.model.description": "Modelo de embedding a ser usado",
    "modules.ai.embed.params.model.label": "Modelo",
    "modules.ai.embed.params.provider.description": "Provedor de IA para embeddings",
    "modules.ai.embed.params.provider.label": "Provedor",
    "modules.ai.embed.params.text.description": "Texto para embutir",
    "modules.ai.embed.params.text.label": "Texto",
    "modules.ai.extract.description": "Extraia dados estruturados de texto usando IA",
    "modules.ai.extract.label": "Extração de IA",
    "modules.ai.extract.output.extracted.description": "Dados estruturados extraídos",
    "modules.ai.extract.output.model.description": "Modelo usado para extração",
    "modules.ai.extract.output.raw_response.description": "Resposta bruta do modelo",
    "modules.ai.extract.params.api_key.description": "Chave de API (usa variável de ambiente como padrão)",
    "modules.ai.extract.params.api_key.label": "Chave de API",
    "modules.ai.extract.params.instructions.description": "Instruções adicionais de extração",
    "modules.ai.extract.params.instructions.label": "Instruções",
    "modules.ai.extract.params.model.description": "Modelo a ser usado para extração",
    "modules.ai.extract.params.model.label": "Modelo",
    "modules.ai.extract.params.provider.description": "Provedor de IA a ser usado",
    "modules.ai.extract.params.provider.label": "Provedor",
    "modules.ai.extract.params.schema.description": "Esquema JSON definindo os campos a serem extraídos",
    "modules.ai.extract.params.schema.label": "Esquema",
    "modules.ai.extract.params.temperature.description": "Temperatura de amostragem (0-2)",
    "modules.ai.extract.params.temperature.label": "Temperatura",
    "modules.ai.extract.params.text.description": "Texto do qual extrair dados",
    "modules.ai.extract.params.text.label": "Texto",
    "modules.ai.local_ollama.chat.description": "Conversar com LLM local via Ollama (completamente offline)",
    "modules.ai.local_ollama.chat.label": "Chat Ollama Local",
    "modules.ai.local_ollama.chat.output.context.description": "Resposta da operacao",
    "modules.ai.local_ollama.chat.output.eval_count.description": "Duracao do carregamento do modelo",
    "modules.ai.local_ollama.chat.output.load_duration.description": "Contexto da conversa para requisicoes de acompanhamento",
    "modules.ai.local_ollama.chat.output.model.description": "Resposta da operacao",
    "modules.ai.local_ollama.chat.output.prompt_eval_count.description": "Duracao total do processamento",
    "modules.ai.local_ollama.chat.output.response.description": "Maximo de tokens na resposta (opcional, depende do modelo)",
    "modules.ai.local_ollama.chat.output.total_duration.description": "Nome ou identificador do modelo",
    "modules.ai.local_ollama.chat.params.max_tokens.description": "URL do servidor Ollama",
    "modules.ai.local_ollama.chat.params.max_tokens.label": "URL Ollama",
    "modules.ai.local_ollama.chat.params.model.description": "A mensagem para enviar ao LLM local",
    "modules.ai.local_ollama.chat.params.model.label": "Prompt",
    "modules.ai.local_ollama.chat.params.ollama_url.description": "Mensagem de role do sistema (opcional)",
    "modules.ai.local_ollama.chat.params.ollama_url.label": "Mensagem do Sistema",
    "modules.ai.local_ollama.chat.params.prompt.description": "A mensagem para enviar ao LLM local",
    "modules.ai.local_ollama.chat.params.prompt.label": "Prompt",
    "modules.ai.local_ollama.chat.params.system_message.description": "Mensagem de role do sistema (opcional)",
    "modules.ai.local_ollama.chat.params.system_message.label": "Temperatura",
    "modules.ai.local_ollama.chat.params.temperature.description": "Temperatura de amostragem (0-2)",
    "modules.ai.local_ollama.chat.params.temperature.label": "CodeLlama 7B",
    "modules.ai.memory.description": "Memoria de conversa para Agente de IA",
    "modules.ai.memory.entity.description": "Extrair e rastrear entidades (pessoas, lugares, conceitos) de conversas",
    "modules.ai.memory.entity.label": "Memoria de Entidades",
    "modules.ai.memory.entity.output.config.description": "Entidades rastreadas por tipo",
    "modules.ai.memory.entity.output.entities.description": "Tipo de memoria (entidade)",
    "modules.ai.memory.entity.output.memory_type.description": "Numero maximo de entidades para lembrar",
    "modules.ai.memory.entity.output.relationships.description": "Identificador da sessao",
    "modules.ai.memory.entity.output.session_id.description": "Numero maximo de entidades para lembrar",
    "modules.ai.memory.entity.params.entity_types": "Tipos de Entidades",
    "modules.ai.memory.entity.params.entity_types.options.concept": "Concept",
    "modules.ai.memory.entity.params.entity_types.options.location": "Location",
    "modules.ai.memory.entity.params.entity_types.options.organization": "Organization",
    "modules.ai.memory.entity.params.entity_types.options.person": "Person",
    "modules.ai.memory.entity.params.extraction_model": "Modelo de Extracao",
    "modules.ai.memory.entity.params.max_entities": "Rastrear Relacionamentos",
    "modules.ai.memory.entity.params.session_id": "ID da Sessao",
    "modules.ai.memory.entity.params.track_relationships": "ID da Sessao",
    "modules.ai.memory.entity.ports.memory": "Tipos de Entidades",
    "modules.ai.memory.label": "Memoria de IA",
    "modules.ai.memory.options.buffer": "Tipo de Memoria",
    "modules.ai.memory.options.summary": "Tamanho da Janela",
    "modules.ai.memory.options.window": "Tipo de Memoria",
    "modules.ai.memory.output.config.description": "Identificador da sessao",
    "modules.ai.memory.output.memory_type.description": "Historico de conversa pre-carregado",
    "modules.ai.memory.output.messages.description": "Tipo de memoria",
    "modules.ai.memory.output.session_id.description": "Historico de conversa pre-carregado",
    "modules.ai.memory.params.initial_messages": "ID da Sessao",
    "modules.ai.memory.params.initial_messages.description": "Historico de conversa pre-carregado",
    "modules.ai.memory.params.memory_type": "Tipo de Memoria",
    "modules.ai.memory.params.memory_type.description": "Tipo de armazenamento de memoria",
    "modules.ai.memory.params.session_id": "ID da Sessao",
    "modules.ai.memory.params.session_id.description": "Identificador unico para esta sessao de conversa",
    "modules.ai.memory.params.window_size": "Tamanho da Janela",
    "modules.ai.memory.params.window_size.description": "Numero de mensagens recentes para manter (para memoria de janela)",
    "modules.ai.memory.ports.memory": "Tipo de Memoria",
    "modules.ai.memory.redis.description": "Memoria de conversa persistente usando armazenamento Redis",
    "modules.ai.memory.redis.label": "Memoria Redis",
    "modules.ai.memory.redis.output.config.description": "Historico de mensagens carregado",
    "modules.ai.memory.redis.output.connected.description": "Identificador da sessao",
    "modules.ai.memory.redis.output.memory_type.description": "Carregar mensagens existentes do Redis na inicializacao",
    "modules.ai.memory.redis.output.messages.description": "Tipo de memoria (redis)",
    "modules.ai.memory.redis.output.session_id.description": "Carregar mensagens existentes do Redis na inicializacao",
    "modules.ai.memory.redis.params.key_prefix": "URL Redis",
    "modules.ai.memory.redis.params.load_on_start": "Max Mensagens",
    "modules.ai.memory.redis.params.max_messages": "TTL (segundos)",
    "modules.ai.memory.redis.params.redis_url": "URL Redis",
    "modules.ai.memory.redis.params.session_id": "Prefixo da Chave",
    "modules.ai.memory.redis.params.ttl_seconds": "ID da Sessao",
    "modules.ai.memory.redis.ports.memory": "URL Redis",
    "modules.ai.memory.vector.description": "Memoria semantica usando embeddings vetoriais para recuperacao de contexto relevante",
    "modules.ai.memory.vector.label": "Memoria Vetorial",
    "modules.ai.memory.vector.output.config.description": "Identificador da sessao",
    "modules.ai.memory.vector.output.embedding_model.description": "Tipo de memoria (vetorial)",
    "modules.ai.memory.vector.output.memory_type.description": "Incluir timestamp e outros metadados com memorias",
    "modules.ai.memory.vector.output.session_id.description": "Incluir timestamp e outros metadados com memorias",
    "modules.ai.memory.vector.params.embedding_model": "Modelo de Embedding",
    "modules.ai.memory.vector.params.include_metadata": "ID da Sessao",
    "modules.ai.memory.vector.params.session_id": "Limite de Similaridade",
    "modules.ai.memory.vector.params.similarity_threshold": "Top K Resultados",
    "modules.ai.memory.vector.params.top_k": "Top K Resultados",
    "modules.ai.memory.vector.ports.memory": "Modelo de Embedding",
    "modules.ai.model.description": "Configuracao de modelo LLM para Agente de IA",
    "modules.ai.model.label": "Modelo de IA",
    "modules.ai.model.output.config.description": "Nome do provedor LLM",
    "modules.ai.model.output.model.description": "Nome do provedor LLM",
    "modules.ai.model.output.provider.description": "Maximo de tokens na resposta",
    "modules.ai.model.params.max_tokens": "Max Tokens",
    "modules.ai.model.params.max_tokens.description": "Maximo de tokens na resposta",
    "modules.ai.model.ports.model": "Modelo",
    "modules.ai.vision.analyze.description": "Analise imagens usando modelos de visão de IA",
    "modules.ai.vision.analyze.label": "Análise de Visão",
    "modules.ai.vision.analyze.output.analysis.description": "Análise de IA da imagem",
    "modules.ai.vision.analyze.output.model.description": "Modelo usado para análise",
    "modules.ai.vision.analyze.output.provider.description": "Provedor usado para análise",
    "modules.ai.vision.analyze.output.tokens_used.description": "Número de tokens usados",
    "modules.ai.vision.analyze.params.api_key.description": "Chave de API (usa variável de ambiente como padrão)",
    "modules.ai.vision.analyze.params.api_key.label": "Chave de API",
    "modules.ai.vision.analyze.params.detail.description": "Nível de detalhe da imagem (baixo/alto/auto)",
    "modules.ai.vision.analyze.params.detail.label": "Detalhe",
    "modules.ai.vision.analyze.params.image_path.description": "Caminho local para o arquivo de imagem",
    "modules.ai.vision.analyze.params.image_path.label": "Caminho da Imagem",
    "modules.ai.vision.analyze.params.image_url.description": "URL da imagem a ser analisada",
    "modules.ai.vision.analyze.params.image_url.label": "URL da Imagem",
    "modules.ai.vision.analyze.params.max_tokens.description": "Máximo de tokens na resposta",
    "modules.ai.vision.analyze.params.max_tokens.label": "Máx. de Tokens",
    "modules.ai.vision.analyze.params.model.description": "Modelo de visão a ser usado",
    "modules.ai.vision.analyze.params.model.label": "Modelo",
    "modules.ai.vision.analyze.params.prompt.description": "O que analisar ou perguntar sobre a imagem",
    "modules.ai.vision.analyze.params.prompt.label": "Prompt",
    "modules.ai.vision.analyze.params.provider.description": "Provedor de IA para análise de visão",
    "modules.ai.vision.analyze.params.provider.label": "Provedor"
  }
}

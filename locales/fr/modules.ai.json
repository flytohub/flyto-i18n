{
  "$schema": "../../schema/locale.schema.json",
  "locale": "fr",
  "category": "ai",
  "version": "1.0.0",
  "translations": {
    "modules.ai.local_ollama.chat.description": "Discuter avec un LLM local via Ollama (completement hors ligne)",
    "modules.ai.local_ollama.chat.label": "Chat Ollama local",
    "modules.ai.local_ollama.chat.output.context.description": "Reponse de l'operation",
    "modules.ai.local_ollama.chat.output.eval_count.description": "Duree de chargement du modele",
    "modules.ai.local_ollama.chat.output.load_duration.description": "Contexte de conversation pour les requetes suivantes",
    "modules.ai.local_ollama.chat.output.model.description": "Reponse de l'operation",
    "modules.ai.local_ollama.chat.output.prompt_eval_count.description": "Duree totale de traitement",
    "modules.ai.local_ollama.chat.output.response.description": "Tokens maximum dans la reponse (optionnel, depend du modele)",
    "modules.ai.local_ollama.chat.output.total_duration.description": "Nom ou identifiant du modele",
    "modules.ai.local_ollama.chat.params.max_tokens.description": "URL du serveur Ollama",
    "modules.ai.local_ollama.chat.params.max_tokens.label": "URL Ollama",
    "modules.ai.local_ollama.chat.params.model.description": "Le message a envoyer au LLM local",
    "modules.ai.local_ollama.chat.params.model.label": "Prompt",
    "modules.ai.local_ollama.chat.params.ollama_url.description": "Message de role systeme (optionnel)",
    "modules.ai.local_ollama.chat.params.ollama_url.label": "Message systeme",
    "modules.ai.local_ollama.chat.params.prompt.description": "Le message a envoyer au LLM local",
    "modules.ai.local_ollama.chat.params.prompt.label": "Prompt",
    "modules.ai.local_ollama.chat.params.system_message.description": "Message de role systeme (optionnel)",
    "modules.ai.local_ollama.chat.params.system_message.label": "Temperature",
    "modules.ai.local_ollama.chat.params.temperature.description": "Temperature d'echantillonnage (0-2)",
    "modules.ai.local_ollama.chat.params.temperature.label": "CodeLlama 7B",
    "modules.ai.memory.description": "Memoire de conversation pour l'agent IA",
    "modules.ai.memory.entity.description": "Extraire et suivre les entites (personnes, lieux, concepts) des conversations",
    "modules.ai.memory.entity.label": "Memoire d'entites",
    "modules.ai.memory.entity.output.config.description": "Entites suivies par type",
    "modules.ai.memory.entity.output.entities.description": "Type de memoire (entite)",
    "modules.ai.memory.entity.output.memory_type.description": "Nombre maximum d'entites a retenir",
    "modules.ai.memory.entity.output.relationships.description": "Identifiant de session",
    "modules.ai.memory.entity.output.session_id.description": "Nombre maximum d'entites a retenir",
    "modules.ai.memory.entity.params.entity_types": "Types d'entites",
    "modules.ai.memory.entity.params.entity_types.options.concept": "Concept",
    "modules.ai.memory.entity.params.entity_types.options.location": "Location",
    "modules.ai.memory.entity.params.entity_types.options.organization": "Organization",
    "modules.ai.memory.entity.params.entity_types.options.person": "Person",
    "modules.ai.memory.entity.params.extraction_model": "Modele d'extraction",
    "modules.ai.memory.entity.params.max_entities": "Suivre les relations",
    "modules.ai.memory.entity.params.session_id": "ID de session",
    "modules.ai.memory.entity.params.track_relationships": "ID de session",
    "modules.ai.memory.entity.ports.memory": "Types d'entites",
    "modules.ai.memory.label": "Memoire IA",
    "modules.ai.memory.options.buffer": "Type de memoire",
    "modules.ai.memory.options.summary": "Taille de la fenetre",
    "modules.ai.memory.options.window": "Type de memoire",
    "modules.ai.memory.output.config.description": "Identifiant de session",
    "modules.ai.memory.output.memory_type.description": "Historique de conversation prechargee",
    "modules.ai.memory.output.messages.description": "Type de memoire",
    "modules.ai.memory.output.session_id.description": "Historique de conversation prechargee",
    "modules.ai.memory.params.initial_messages": "ID de session",
    "modules.ai.memory.params.initial_messages.description": "Historique de conversation prechargee",
    "modules.ai.memory.params.memory_type": "Type de memoire",
    "modules.ai.memory.params.memory_type.description": "Type de stockage memoire",
    "modules.ai.memory.params.session_id": "ID de session",
    "modules.ai.memory.params.session_id.description": "Identifiant unique pour cette session de conversation",
    "modules.ai.memory.params.window_size": "Taille de la fenetre",
    "modules.ai.memory.params.window_size.description": "Nombre de messages recents a conserver (pour la memoire fenetre)",
    "modules.ai.memory.ports.memory": "Type de memoire",
    "modules.ai.memory.redis.description": "Memoire de conversation persistante utilisant le stockage Redis",
    "modules.ai.memory.redis.label": "Memoire Redis",
    "modules.ai.memory.redis.output.config.description": "Historique des messages charges",
    "modules.ai.memory.redis.output.connected.description": "Identifiant de session",
    "modules.ai.memory.redis.output.memory_type.description": "Charger les messages existants de Redis a l'initialisation",
    "modules.ai.memory.redis.output.messages.description": "Type de memoire (redis)",
    "modules.ai.memory.redis.output.session_id.description": "Charger les messages existants de Redis a l'initialisation",
    "modules.ai.memory.redis.params.key_prefix": "URL Redis",
    "modules.ai.memory.redis.params.load_on_start": "Messages max",
    "modules.ai.memory.redis.params.max_messages": "TTL (secondes)",
    "modules.ai.memory.redis.params.redis_url": "URL Redis",
    "modules.ai.memory.redis.params.session_id": "Prefixe de cle",
    "modules.ai.memory.redis.params.ttl_seconds": "ID de session",
    "modules.ai.memory.redis.ports.memory": "URL Redis",
    "modules.ai.memory.vector.description": "Memoire semantique utilisant des embeddings vectoriels pour la recuperation de contexte pertinent",
    "modules.ai.memory.vector.label": "Memoire vectorielle",
    "modules.ai.memory.vector.output.config.description": "Identifiant de session",
    "modules.ai.memory.vector.output.embedding_model.description": "Type de memoire (vecteur)",
    "modules.ai.memory.vector.output.memory_type.description": "Inclure l'horodatage et autres metadonnees avec les memoires",
    "modules.ai.memory.vector.output.session_id.description": "Inclure l'horodatage et autres metadonnees avec les memoires",
    "modules.ai.memory.vector.params.embedding_model": "Modele d'embedding",
    "modules.ai.memory.vector.params.include_metadata": "ID de session",
    "modules.ai.memory.vector.params.session_id": "Seuil de similarite",
    "modules.ai.memory.vector.params.similarity_threshold": "Top K resultats",
    "modules.ai.memory.vector.params.top_k": "Top K resultats",
    "modules.ai.memory.vector.ports.memory": "Modele d'embedding",
    "modules.ai.model.description": "Configuration du modele LLM pour l'agent IA",
    "modules.ai.model.label": "Modele IA",
    "modules.ai.model.output.config.description": "Nom du fournisseur LLM",
    "modules.ai.model.output.model.description": "Nom du fournisseur LLM",
    "modules.ai.model.output.provider.description": "Tokens maximum dans la reponse",
    "modules.ai.model.params.max_tokens": "Tokens max",
    "modules.ai.model.params.max_tokens.description": "Tokens maximum dans la reponse",
    "modules.ai.model.ports.model": "Modele"
  }
}
